import os
import re
import time
import logging
import threading
import requests
import sqlite3
import hashlib
from datetime import datetime, timezone
from http.server import HTTPServer, BaseHTTPRequestHandler
from bs4 import BeautifulSoup
from deep_translator import GoogleTranslator, MyMemoryTranslator

# ============= –ù–ê–°–¢–†–û–ô–ö–ò =============
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN", "")
# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∑–∞–¥–∞–Ω—ã –¥–≤–∞ –∫–∞–Ω–∞–ª–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
CHANNEL_IDS = [cid.strip() for cid in os.getenv("@time_n_John", "@finanosint").split(",") if cid.strip()]
DRY_RUN = os.getenv("DRY_RUN", "0") == "1"

# –°–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ ‚Äî —É–±—Ä–∞–Ω—ã –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ URL
SOURCES = [
    {"name": "Good Judgment", "url": "https://goodjudgment.com/feed/"},
    {"name": "Johns Hopkins", "url": "https://www.centerforhealthsecurity.org/feed.xml"},
    {"name": "Metaculus", "url": "https://www.metaculus.com/feed/"},
    {"name": "DNI Global Trends", "url": "https://www.dni.gov/index.php/gt2040-home?format=feed&type=rss"},
    {"name": "RAND Corporation", "url": "https://www.rand.org/rss.xml"},
    {"name": "World Economic Forum", "url": "https://www.weforum.org/en/feeds/rss"},
    {"name": "CSIS", "url": "https://www.csis.org/rss.xml"},
    {"name": "Atlantic Council", "url": "https://www.atlanticcouncil.org/feed/"},
    {"name": "Chatham House", "url": "https://www.chathamhouse.org/feeds/all"},
    {"name": "The Economist", "url": "https://www.economist.com/the-world-this-week/rss.xml"},
    {"name": "Bloomberg Politics", "url": "https://www.bloomberg.com/politics/feeds/site.xml"},
    {"name": "Foreign Affairs", "url": "https://www.foreignaffairs.com/rss.xml"},
    {"name": "CFR", "url": "https://www.cfr.org/rss"},
    {"name": "BBC Future", "url": "https://feeds.bbci.co.uk/news/science_and_environment/rss.xml"},
    {"name": "Future Timeline", "url": "https://www.futuretimeline.net/feed/"},
    {"name": "Carnegie Endowment", "url": "https://carnegieendowment.org/rss.xml"},
    {"name": "Bruegel", "url": "https://www.bruegel.org/rss.xml"},
    {"name": "E3G", "url": "https://www.e3g.org/feed/"},
]

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
KEYWORDS = [
   r"\brussia\b", r"\brussian\b", r"\bputin\b", r"\bmoscow\b", r"\bkremlin\b", r"\bukraine\b", r"\bukrainian\b", r"\bzelensky\b", r"\bkyiv\b", r"\bkiev\b", r"\bcrimea\b", r"\bdonbas\b", r"\bsanction[s]?\b", r"\bgazprom\b", r"\bnord\s?stream\b", r"\bwagner\b", r"\blavrov\b", r"\bshoigu\b", r"\bmedvedev\b", r"\bpeskov\b", r"\bnato\b", r"\beuropa\b", r"\busa\b", r"\bsoviet\b", r"\bussr\b", r"\bpost\W?soviet\b", r"\bsvo\b", r"\b—Å–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏—è\b", r"\bspecial military operation\b", r"\b–≤–æ–π–Ω–∞\b", r"\bwar\b", r"\bconflict\b", r"\b–∫–æ–Ω—Ñ–ª–∏–∫—Ç\b", r"\b–Ω–∞—Å—Ç—É–ø–ª–µ–Ω–∏–µ\b", r"\boffensive\b", r"\b–∞—Ç–∞–∫–∞\b", r"\battack\b", r"\b—É–¥–∞—Ä\b", r"\bstrike\b", r"\b–æ–±—Å—Ç—Ä–µ–ª\b", r"\bshelling\b", r"\b–¥—Ä–æ–Ω\b", r"\bdrone\b", r"\bmissile\b", r"\b—Ä–∞–∫–µ—Ç–∞\b",  r"\b—ç—Å–∫–∞–ª–∞—Ü–∏—è\b", r"\bescalation\b", r"\b–º–æ–±–∏–ª–∏–∑–∞—Ü–∏—è\b", r"\bmobilization\b", r"\b—Ñ—Ä–æ–Ω—Ç\b", r"\bfrontline\b", r"\b–∑–∞—Ö–≤–∞—Ç\b", r"\bcapture\b", r"\b–æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ\b", r"\bliberation\b", r"\b–±–æ–π\b", r"\bbattle\b", r"\b–ø–æ—Ç–µ—Ä–∏\b", r"\bcasualties\b", r"\b–ø–æ–≥–∏–±\b", r"\bkilled\b", r"\b—Ä–∞–Ω–µ–Ω\b", r"\binjured\b", r"\b–ø–ª–µ–Ω–Ω—ã–π\b", r"\bprisoner of war\b", r"\b–ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã\b", r"\btalks\b", r"\b–ø–µ—Ä–µ–º–∏—Ä–∏–µ\b", r"\bceasefire\b", r"\b—Å–∞–Ω–∫—Ü–∏–∏\b", r"\bsanctions\b", r"\b–æ—Ä—É–∂–∏–µ\b", r"\bweapons\b", r"\b–ø–æ—Å—Ç–∞–≤–∫–∏\b", r"\bsupplies\b", r"\bhimars\b", r"\batacms\b", r"\bhour ago\b", r"\b—á–∞—Å –Ω–∞–∑–∞–¥\b", r"\bminutos atr√°s\b", r"\bÂ∞èÊó∂Ââç\b", r"\bbitcoin\b", r"\bbtc\b", r"\b–±–∏—Ç–∫–æ–∏–Ω\b", r"\bÊØîÁâπÂ∏Å\b", r"\bethereum\b", r"\beth\b", r"\b—ç—Ñ–∏—Ä\b", r"\b‰ª•Â§™Âùä\b", r"\bbinance coin\b", r"\bbnb\b", r"\busdt\b", r"\btether\b", r"\bxrp\b", r"\bripple\b", r"\bcardano\b", r"\bada\b", r"\bsolana\b", r"\bsol\b", r"\bdoge\b", r"\bdogecoin\b", r"\bavalanche\b", r"\bavax\b", r"\bpolkadot\b", r"\bdot\b", r"\bchainlink\b", r"\blink\b", r"\btron\b", r"\btrx\b", r"\bcbdc\b", r"\bcentral bank digital currency\b", r"\b—Ü–∏—Ñ—Ä–æ–≤–æ–π —Ä—É–±–ª—å\b", r"\bdigital yuan\b", r"\beuro digital\b", r"\bdefi\b", r"\b–¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏–Ω–∞–Ω—Å—ã\b", r"\bnft\b", r"\bnon-fungible token\b", r"\bsec\b", r"\b—Ü–± —Ä—Ñ\b", r"\b—Ä–µ–≥—É–ª—è—Ü–∏—è\b", r"\bregulation\b", r"\b–∑–∞–ø—Ä–µ—Ç\b", r"\bban\b", r"\b–º–∞–π–Ω–∏–Ω–≥\b", r"\bmining\b", r"\bhalving\b", r"\b—Ö–∞–ª–≤–∏–Ω–≥\b", r"\b–≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å\b", r"\bvolatility\b", r"\bcrash\b", r"\b–∫—Ä–∞—Ö\b", r"\bÂàöÂàö\b", r"\bÿØŸÇÿßÿ¶ŸÇ ŸÖÿ∂ÿ™\b", r"\bpandemic\b", r"\b–ø–∞–Ω–¥–µ–º–∏—è\b", r"\bÁñ´ÊÉÖ\b", r"\bÿ¨ÿßÿ¶ÿ≠ÿ©\b", r"\boutbreak\b", r"\b–≤—Å–ø—ã—à–∫–∞\b", r"\b—ç–ø–∏–¥–µ–º–∏—è\b", r"\bepidemic\b", r"\bvirus\b", r"\b–≤–∏—Ä—É—Å\b", r"\b–≤–∏—Ä—É—Å—ã\b", r"\bÂèòÂºÇÊ†™\b",  r"\bvaccine\b", r"\b–≤–∞–∫—Ü–∏–Ω–∞\b", r"\bÁñ´Ëãó\b", r"\bŸÑŸÇÿßÿ≠\b", r"\bbooster\b", r"\b–±—É—Å—Ç–µ—Ä\b", r"\b—Ä–µ–≤–∞–∫—Ü–∏–Ω–∞—Ü–∏—è\b", r"\bquarantine\b", r"\b–∫–∞—Ä–∞–Ω—Ç–∏–Ω\b", r"\bÈöîÁ¶ª\b", r"\bÿ≠ÿ¨ÿ± ÿµÿ≠Ÿä\b", r"\blockdown\b", r"\b–ª–æ–∫–¥–∞—É–Ω\b", r"\bÂ∞ÅÈîÅ\b", r"\bmutation\b", r"\b–º—É—Ç–∞—Ü–∏—è\b", r"\bÂèòÂºÇ\b", r"\bstrain\b", r"\b—à—Ç–∞–º–º\b", r"\bomicron\b", r"\bdelta\b", r"\bbiosafety\b", r"\b–±–∏–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å\b", r"\bÁîüÁâ©ÂÆâÂÖ®\b", r"\blab leak\b", r"\b–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —É—Ç–µ—á–∫–∞\b", r"\bÂÆûÈ™åÂÆ§Ê≥ÑÊºè\b", r"\bgain of function\b", r"\b—É—Å–∏–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏\b", r"\bwho\b", r"\b–≤–æ–∑\b", r"\bcdc\b", r"\b—Ä–æ—Å–ø–æ—Ç—Ä–µ–±–Ω–∞–¥–∑–æ—Ä\b", r"\binfection rate\b", r"\b–∑–∞—Ä–∞–∑–Ω–æ—Å—Ç—å\b", r"\bÊ≠ª‰∫°Áéá\b", r"\bhospitalization\b", r"\b–≥–æ—Å–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è\b", r"\bŸÇÿ®ŸÑ ÿ≥ÿßÿπÿßÿ™\b", r"\bÂàöÂàöÊä•Âëä\b"]

DB_PATH = "seen_titles.db"
INTERVAL_SEC = 180
MAX_DB_SIZE = 5000
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"

# ============= –õ–û–ì–ò–†–û–í–ê–ù–ò–ï =============
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
log = logging.getLogger()

# ============= –ë–ê–ó–ê –î–ê–ù–ù–´–• =============
def init_db():
    with sqlite3.connect(DB_PATH) as conn:
        conn.execute("""
            CREATE TABLE IF NOT EXISTS seen_titles (
                title_hash TEXT PRIMARY KEY,
                processed_at TEXT NOT NULL
            )
        """)
        conn.commit()

def normalize_title(title: str) -> str:
    return re.sub(r"[^a-zA-Z–∞-—è–ê-–Ø0-9—ë–Å]", "", title.lower()).strip()

def is_title_seen(title: str) -> bool:
    norm = normalize_title(title)
    if not norm:
        return False
    h = hashlib.sha256(norm.encode("utf-8")).hexdigest()
    with sqlite3.connect(DB_PATH) as conn:
        cur = conn.execute("SELECT 1 FROM seen_titles WHERE title_hash = ?", (h,))
        return cur.fetchone() is not None

def mark_title_seen(title: str):
    norm = normalize_title(title)
    if not norm:
        return
    h = hashlib.sha256(norm.encode("utf-8")).hexdigest()
    now = datetime.now(timezone.utc).isoformat()
    with sqlite3.connect(DB_PATH) as conn:
        conn.execute(
            "INSERT OR IGNORE INTO seen_titles (title_hash, processed_at) VALUES (?, ?)",
            (h, now)
        )
        conn.execute(f"""
            DELETE FROM seen_titles
            WHERE title_hash NOT IN (
                SELECT title_hash FROM seen_titles
                ORDER BY processed_at DESC
                LIMIT {MAX_DB_SIZE}
            )
        """)
        conn.commit()

# ============= –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò =============
def clean_text(t: str) -> str:
    return re.sub(r"\s+", " ", t).strip()

def translate_to_russian(text: str) -> str:
    if not text or not text.strip():
        return ""
    try:
        # –ü–æ–ø—ã—Ç–∫–∞ —Å GoogleTranslator
        result = GoogleTranslator(source='auto', target='ru').translate(text)
        if result and result.strip():
            return result.strip()
    except Exception as e1:
        log.warning(f"GoogleTranslator failed: {e1}")
    
    try:
        # –ü–æ–ø—ã—Ç–∫–∞ —Å MyMemoryTranslator
        result = MyMemoryTranslator(source='auto', target='ru').translate(text)
        if result and result.strip():
            return result.strip()
    except Exception as e2:
        log.warning(f"MyMemoryTranslator also failed: {e2}")
    
    # –ï—Å–ª–∏ –æ–±–∞ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
    return text.strip()

def html_escape(text: str) -> str:
    """–≠–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ HTML-—Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    return text.replace("&", "&amp;").replace("<", "<").replace(">", ">")

# ============= –ü–û–õ–£–ß–ï–ù–ò–ï –ù–û–í–û–°–¢–ï–ô =============
def fetch_news():
    items = []
    headers = {"User-Agent": USER_AGENT}
    
    for src in SOURCES:
        try:
            log.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞: {src['name']}")
            resp = requests.get(src["url"], headers=headers, timeout=15)
            
            if resp.status_code != 200:
                log.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {src['name']}: HTTP {resp.status_code}")
                continue
                
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π –ø–∞—Ä—Å–µ—Ä "xml"
                soup = BeautifulSoup(resp.content, "xml")
            except Exception as e:
                log.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å XML –¥–ª—è {src['name']} —Å lxml: {e}. –ü—Ä–æ–±—É–µ–º html.parser.")
                soup = BeautifulSoup(resp.content, "html.parser")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤–æ–æ–±—â–µ —ç–ª–µ–º–µ–Ω—Ç—ã item
            items_found = soup.find_all("item")
            if not items_found:
                # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ entry –¥–ª—è Atom —Ñ–∏–¥–æ–≤
                items_found = soup.find_all("entry")
            
            if not items_found:
                log.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω—ã —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–æ–≤–æ—Å—Ç–µ–π –≤ {src['name']}")
                continue
                
            for item in items_found:
                title = ""
                link = ""
                desc = ""
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ RSS —Ñ–æ—Ä–º–∞—Ç–∞
                if item.name == "item":
                    title = clean_text(item.title.get_text()) if item.title else ""
                    # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫—É –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ –∞—Ç—Ä–∏–±—É—Ç–∞
                    link = ""
                    if item.link:
                        if item.link.get_text().strip():
                            link = item.link.get_text().strip()
                        elif item.link.get("href"):
                            link = item.link.get("href", "").strip()
                    
                    # –ü–æ–∏—Å–∫ –æ–ø–∏—Å–∞–Ω–∏—è
                    desc_tag = item.find("description") or item.find("content:encoded") or item.find("content")
                    if desc_tag:
                        raw = desc_tag.get_text()
                        # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏
                        clean_desc = BeautifulSoup(raw, "html.parser").get_text()
                        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏–ª–∏ –ø–µ—Ä–≤—ã–µ 250 —Å–∏–º–≤–æ–ª–æ–≤
                        sentences = re.split(r'(?<=[.!?])\s+', clean_desc.strip())
                        desc = sentences[0] if sentences else clean_desc[:250]
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ Atom —Ñ–æ—Ä–º–∞—Ç–∞
                elif item.name == "entry":
                    title = clean_text(item.title.get_text()) if item.title else ""
                    link_tag = item.find("link", rel="alternate") or item.find("link")
                    link = link_tag.get("href", "") if link_tag else ""
                    
                    # –ü–æ–∏—Å–∫ –æ–ø–∏—Å–∞–Ω–∏—è
                    desc_tag = item.find("summary") or item.find("content")
                    if desc_tag:
                        raw = desc_tag.get_text()
                        clean_desc = BeautifulSoup(raw, "html.parser").get_text()
                        sentences = re.split(r'(?<=[.!?])\s+', clean_desc.strip())
                        desc = sentences[0] if sentences else clean_desc[:250]
                
                if not title or not link:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –≤–∏–¥–µ–ª–∏ –ª–∏ –º—ã —ç—Ç—É –Ω–æ–≤–æ—Å—Ç—å —Ä–∞–Ω–µ–µ
                if is_title_seen(title):
                    continue
                
                # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω–æ–µ –î–û —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                mark_title_seen(title)
                
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                if not any(re.search(kw, title, re.IGNORECASE) for kw in KEYWORDS):
                    continue
                
                # –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ –æ–ø–∏—Å–∞–Ω–∏—è
                ru_title = translate_to_russian(title)
                ru_desc = translate_to_russian(desc) if desc else ""
                
                # –≠–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ HTML
                safe_title = html_escape(ru_title)
                safe_desc = html_escape(ru_desc)
                safe_link = html_escape(link)
                
                # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ HTML —Ñ–æ—Ä–º–∞—Ç–µ
                source_bold = f"<b>{src['name']}</b>"
                msg = f"{source_bold}\n\n{safe_title}\n\n{safe_desc}\n\n[–ò—Å—Ç–æ—á–Ω–∏–∫]({link})"
                items.append((msg, title))
                items.append((msg, title))
                
        except Exception as e:
            log.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {src['name']}: {e}")
    
    return items

# ============= –û–¢–ü–†–ê–í–ö–ê –í TELEGRAM =============
def send_to_telegram(text: str, channel_ids: list) -> bool:
    if DRY_RUN:
        log.info(f"[–¢–ï–°–¢] –°–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏:\n{text}\n")
        return True
    
    success = True
    for ch_id in channel_ids:
        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: —É–±—Ä–∞–Ω—ã –ø—Ä–æ–±–µ–ª—ã –≤ URL API
        url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
        payload = {
            "chat_id": ch_id,
            "text": text,
            "parse_mode": "HTML",
            "disable_web_page_preview": True,
        }
        try:
            r = requests.post(url, data=payload, timeout=15)
            if r.status_code != 200:
                log.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –≤ {ch_id}: {r.text}")
                success = False
            else:
                log.info(f"‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ {ch_id}")
        except Exception as e:
            log.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ {ch_id}: {e}")
            success = False
        time.sleep(0.5)  # –∏–∑–±–µ–≥–∞–µ–º rate limit Telegram API
    
    return success

# ============= HEALTH CHECK =============
class HealthHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b"OK")
    def log_message(self, format, *args):
        pass

def start_health_server():
    port = int(os.environ.get("PORT", 10000))
    server = HTTPServer(("0.0.0.0", port), HealthHandler)
    log.info(f"Health check server –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É {port}")
    server.serve_forever()

# ============= –û–°–ù–û–í–ù–û–ô –¶–ò–ö–õ =============
def main_loop():
    init_db()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if not TELEGRAM_TOKEN:
        log.error("‚ùå TELEGRAM_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
        return
    
    if not CHANNEL_IDS:
        log.error("‚ùå CHANNEL_IDS –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! –ü—Ä–∏–º–µ—Ä: CHANNEL_IDS=@channel1,@channel2")
        return
    
    log.info(f"üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω. –ö–∞–Ω–∞–ª—ã: {', '.join(CHANNEL_IDS)}")
    if DRY_RUN:
        log.info("üß™ –†–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (DRY_RUN) –≤–∫–ª—é—á–µ–Ω - —Å–æ–æ–±—â–µ–Ω–∏—è –Ω–µ –±—É–¥—É—Ç –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å—Å—è")
    
    while True:
        try:
            news = fetch_news()
            sent = 0
            total = len(news)
            
            for msg, orig_title in news:
                if send_to_telegram(msg, CHANNEL_IDS):
                    sent += 1
                time.sleep(1)
            
            log.info(f"‚úÖ –¶–∏–∫–ª –∑–∞–≤–µ—Ä—à—ë–Ω. –ù–∞–π–¥–µ–Ω–æ: {total}, –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {sent}")
        except Exception as e:
            log.exception(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {e}")
        time.sleep(INTERVAL_SEC)

# ============= –ó–ê–ü–£–°–ö =============
if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ health check —Å–µ—Ä–≤–µ—Ä–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    threading.Thread(target=start_health_server, daemon=True).start()
    
    # –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
    main_loop()


