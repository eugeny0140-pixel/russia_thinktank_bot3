# russia_thinktank_bot.py
import os
import re
import time
import logging
import requests
from bs4 import BeautifulSoup
from deep_translator import GoogleTranslator
import schedule
from dotenv import load_dotenv

load_dotenv()

TELEGRAM_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID", "@time_n_John")

if not TELEGRAM_TOKEN:
    raise ValueError("âŒ TELEGRAM_BOT_TOKEN Ð½Ðµ Ð·Ð°Ð´Ð°Ð½")

SOURCES = [
    {"name": "Foreign Affairs", "url": "https://www.foreignaffairs.com/rss.xml"},
    {"name": "Reuters Institute", "url": "https://reutersinstitute.politics.ox.ac.uk/rss.xml"},
    {"name": "Bruegel", "url": "https://www.bruegel.org/rss.xml"},
    {"name": "E3G", "url": "https://www.e3g.org/feed/"},
    {"name": "Chatham House", "url": "https://www.chathamhouse.org/rss.xml"},
    {"name": "CSIS", "url": "https://www.csis.org/rss.xml"},
    {"name": "Atlantic Council", "url": "https://www.atlanticcouncil.org/feed/"},
    {"name": "RAND Corporation", "url": "https://www.rand.org/rss.xml"},
    {"name": "CFR", "url": "https://www.cfr.org/rss/"},
    {"name": "Carnegie Endowment", "url": "https://carnegieendowment.org/rss.xml"},
    {"name": "The Economist", "url": "https://www.economist.com/latest/rss.xml"},
    {"name": "Bloomberg Politics", "url": "https://www.bloomberg.com/politics/feeds/site.xml"},
]

KEYWORDS = [
    r"\brussia\b", r"\brussian\b", r"\bputin\b", r"\bmoscow\b", r"\bkremlin\b",
    r"\bukraine\b", r"\bukrainian\b", r"\bzelensky\b", r"\bkyiv\b", r"\bkiev\b",
    r"\bcrimea\b", r"\bdonbas\b", r"\bsanction[s]?\b", r"\bgazprom\b",
    r"\bnord\s?stream\b", r"\bwagner\b", r"\blavrov\b", r"\bshoigu\b",
    r"\bmedvedev\b", r"\bpeskov\b", r"\bnato\b", r"\beuropa\b", r"\busa\b",
    r"\bwar\b", r"\bconflict\b", r"\bmilitary\b", r"\bruble\b", r"\beconomy\b",
    r"\benergy\b", r"\boil\b", r"\bgas\b", r"\bsoviet\b", r"\bpost\W?soviet\b"
]

MAX_SEEN = 5000
MAX_PER_RUN = 12
seen_links = set()

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
log = logging.getLogger(__name__)

def clean_text(t):
    return re.sub(r"\s+", " ", t).strip()

def translate_to_russian(text):
    try:
        return GoogleTranslator(source='auto', target='ru').translate(text)
    except:
        return text

def get_summary(title):
    low = title.lower()
    if re.search(r"sanction|embargo|restrict", low):
        return "Ð’Ð²ÐµÐ´ÐµÐ½Ñ‹ Ð½Ð¾Ð²Ñ‹Ðµ ÑÐ°Ð½ÐºÑ†Ð¸Ð¸ Ð¸Ð»Ð¸ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ."
    if re.search(r"war|attack|strike|bomb|conflict|military", low):
        return "Ð¡Ð¾Ð¾Ð±Ñ‰Ð°ÐµÑ‚ÑÑ Ð¾ Ð²Ð¾ÐµÐ½Ð½Ñ‹Ñ… Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸ÑÑ… Ð¸Ð»Ð¸ ÑƒÐ´Ð°Ñ€Ð°Ñ…."
    if re.search(r"putin|kremlin|peskov|moscow", low):
        return "Ð—Ð°ÑÐ²Ð»ÐµÐ½Ð¸Ðµ Ð¸Ð»Ð¸ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ ÑÐ¾ ÑÑ‚Ð¾Ñ€Ð¾Ð½Ñ‹ ÐšÑ€ÐµÐ¼Ð»Ñ."
    if re.search(r"economy|rubl?e|oil|gas|gazprom|nord\s?stream|energy", low):
        return "ÐÐ¾Ð²Ð¾ÑÑ‚Ð¸ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸ÐºÐ¸, Ð½ÐµÑ„Ñ‚Ð¸, Ð³Ð°Ð·Ð° Ð¸Ð»Ð¸ Ñ€ÑƒÐ±Ð»Ñ."
    if re.search(r"diplomat|talks|negotiat|meeting|lavrov", low):
        return "Ð”Ð¸Ð¿Ð»Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¿ÐµÑ€ÐµÐ³Ð¾Ð²Ð¾Ñ€Ñ‹ Ð¸Ð»Ð¸ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ñ‹."
    if re.search(r"wagner|shoigu|medvedev|defense", low):
        return "Ð¡Ð¾Ð±Ñ‹Ñ‚Ð¸Ñ Ñ Ñ€Ð¾ÑÑÐ¸Ð¹ÑÐºÐ¸Ð¼Ð¸ Ð²Ð¾ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð¸Ð»Ð¸ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸ÐºÐ°Ð¼Ð¸."
    if re.search(r"ukraine|zelensky|kyiv|kiev|crimea|donbas", low):
        return "Ð¡Ð¾Ð±Ñ‹Ñ‚Ð¸Ñ, ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ðµ Ñ Ð£ÐºÑ€Ð°Ð¸Ð½Ð¾Ð¹ Ð¸ Ð¿Ñ€Ð¸Ð»ÐµÐ³Ð°ÑŽÑ‰Ð¸Ð¼Ð¸ Ñ€ÐµÐ³Ð¸Ð¾Ð½Ð°Ð¼Ð¸."
    if re.search(r"nato|europa|european|germany|france|usa|uk", low):
        return "Ð ÐµÐ°ÐºÑ†Ð¸Ñ Ð·Ð°Ð¿Ð°Ð´Ð½Ñ‹Ñ… ÑÑ‚Ñ€Ð°Ð½ Ð¸Ð»Ð¸ ÐÐÐ¢Ðž Ð½Ð° ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ Ñ ÑƒÑ‡Ð°ÑÑ‚Ð¸ÐµÐ¼ Ð Ð¾ÑÑÐ¸Ð¸."
    return "ÐÐ½Ð°Ð»Ð¸Ñ‚Ð¸ÐºÐ°, ÑÐ²ÑÐ·Ð°Ð½Ð½Ð°Ñ Ñ Ð Ð¾ÑÑÐ¸ÐµÐ¹ Ð¸Ð»Ð¸ Ð¿Ð¾ÑÑ‚ÑÐ¾Ð²ÐµÑ‚ÑÐºÐ¸Ð¼ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ð¾Ð¼."

def get_source_prefix(name):
    name = name.lower()
    mapping = {
        "e3g": "e3g",
        "foreign affairs": "foreignaffairs",
        "chatham house": "chathamhouse",
        "csis": "csis",
        "atlantic council": "atlanticcouncil",
        "rand": "rand",
        "cfr": "cfr",
        "carnegie": "carnegie",
        "bruegel": "bruegel",
        "bloomberg": "bloomberg",
        "reuters institute": "reuters",
        "the economist": "economist"
    }
    for key, val in mapping.items():
        if key in name:
            return val
    return name.split()[0].lower()

def escape_markdown_v2(text):
    # Ð­ÐºÑ€Ð°Ð½Ð¸Ñ€ÑƒÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑÐ¸Ð¼Ð²Ð¾Ð»Ñ‹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð»Ð¾Ð¼Ð°ÑŽÑ‚ MarkdownV2
    return text.replace('_', '\\_').replace('*', '\\*').replace('[', '\\[').replace(']', '\\]')

def fetch_rss_news():
    global seen_links
    result = []
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

    for src in SOURCES:
        if len(result) >= MAX_PER_RUN:
            break
        try:
            url = src["url"].strip()
            log.info(f"ðŸ“¡ {src['name']}")
            resp = requests.get(url, timeout=30, headers=headers)
            soup = BeautifulSoup(resp.content, "xml")

            for item in soup.find_all("item"):
                if len(result) >= MAX_PER_RUN:
                    break

                title = clean_text(item.title.get_text()) if item.title else ""
                link = (item.link.get_text() or item.guid.get_text()).strip() if item.link or item.guid else ""

                if not title or not link or link in seen_links:
                    continue

                if not any(re.search(kw, title, re.IGNORECASE) for kw in KEYWORDS):
                    continue

                # === ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ ===
                description = ""
                desc_tag = item.find("description") or item.find("content:encoded")
                if desc_tag:
                    raw_html = desc_tag.get_text()
                    desc_text = clean_text(BeautifulSoup(raw_html, "html.parser").get_text())

                    # ÐžÑ‚ÑÐµÐ¸Ð²Ð°ÐµÐ¼ ÑˆÐ°Ð±Ð»Ð¾Ð½Ð½Ñ‹Ðµ Ñ„Ñ€Ð°Ð·Ñ‹
                    if not re.search(
                        r"(?i)appeared first on|this article was|originally published|post.*appeared|insight-impact",
                        desc_text
                    ):
                        if len(desc_text) > 400:
                            description = desc_text[:400].rsplit(' ', 1)[0] + "â€¦"
                        else:
                            description = desc_text

                if not description.strip():
                    description = get_summary(title)

                # ÐŸÐµÑ€ÐµÐ²Ð¾Ð´Ð¸Ð¼
                ru_title = translate_to_russian(title)
                description_ru = translate_to_russian(description)

                # Ð­ÐºÑ€Ð°Ð½Ð¸Ñ€ÑƒÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ (Ð½Ðµ ÑÑÑ‹Ð»ÐºÑƒ!)
                safe_title = escape_markdown_v2(ru_title)
                safe_desc = escape_markdown_v2(description_ru)

                prefix = get_source_prefix(src["name"])
                msg = f"{prefix}: {safe_title}\n\n{safe_desc}\n\n[Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº]({link})"
                result.append({"msg": msg, "link": link})

        except Exception as e:
            log.error(f"âŒ {src['name']}: {e}")

    return result

def send_to_telegram(text):
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    payload = {
        "chat_id": CHANNEL_ID,
        "text": text,
        "parse_mode": "MarkdownV2",  # Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ MarkdownV2 Ð´Ð»Ñ Ð½Ð°Ð´Ñ‘Ð¶Ð½Ð¾ÑÑ‚Ð¸
        "disable_web_page_preview": True,
    }
    try:
        r = requests.post(url, data=payload, timeout=15)
        log.info("âœ… ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¾" if r.status_code == 200 else f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ°: {r.text}")
    except Exception as e:
        log.error(f"âŒ Ð˜ÑÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ: {e}")

def job():
    global seen_links
    log.info("ðŸ”„ ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð¾Ð²Ð¾ÑÑ‚ÐµÐ¹...")
    news = fetch_rss_news()
    if not news:
        log.info("ðŸ“­ ÐÐµÑ‚ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ñ… Ð¿ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¹.")
        return

    for item in news:
        send_to_telegram(item["msg"])
        seen_links.add(item["link"])
        if len(seen_links) > MAX_SEEN:
            seen_links = set(list(seen_links)[-4000:])
        time.sleep(1)

if __name__ == "__main__":
    log.info("ðŸš€ Ð‘Ð¾Ñ‚ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½")
    job()
    schedule.every(30).minutes.do(job)
    while True:
        schedule.run_pending()
        time.sleep(1)